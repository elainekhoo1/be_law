{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.add('also')\n",
    "stops.add(\"note\")\n",
    "stops.add(\"notes\")\n",
    "stops.add(\"instance\")\n",
    "stops.add(\"example\")\n",
    "stops.add(\"\")\n",
    "\n",
    "\n",
    "class TextPreProcessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def text_normalizer(self, text):\n",
    "        normalized_text = text.lower()\n",
    "        normalized_text = re.sub(r\"_-\\'\\.\", '', normalized_text)\n",
    "        normalized_text = re.sub(r\"\\n,\", ' ', normalized_text)\n",
    "        normalized_text = re.sub(r\"[^a-z ]\", ' ', normalized_text)\n",
    "        normalized_text = re.sub(r\"[ ]+\", ' ', normalized_text)\n",
    "\n",
    "        return normalized_text\n",
    "\n",
    "    def tokenize(self, text):  return word_tokenize(text)\n",
    "\n",
    "    def stem_lemmatize_stopwordremoval(self, tokens):\n",
    "        newTokens = set(tokens) - stops\n",
    "#         newTokens = list(map(stemmer.stem, newTokens))\n",
    "        newTokens = list(map(lemmatizer.lemmatize, newTokens))\n",
    "        newTokens = [token for token in newTokens if len(token) >= 3]\n",
    "\n",
    "        return newTokens\n",
    "\n",
    "    def preProcessIt(self, text):\n",
    "        normalized_text = self.text_normalizer(text)\n",
    "        preporcessed_text = self.stem_lemmatize_stopwordremoval(self.tokenize(normalized_text))\n",
    "        preporcessed_text = \" \".join(preporcessed_text)\n",
    "\n",
    "        return preporcessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"data\\crime_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesser = TextPreProcessor()\n",
    "df[\"preprocessed_text\"] = df[\"news_article\"].apply(preprocesser.preProcessIt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_article</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICE INVESTIGATE ARSON CLAIM IN FIRE THAT TO...</td>\n",
       "      <td>arson</td>\n",
       "      <td>week several gunung people told work white hos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAPAN: KINKAKUJI TEMPLE HOLDS CEREMONY RECALLI...</td>\n",
       "      <td>arson</td>\n",
       "      <td>kyodo day old july hold monk official charge t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NASI KANDAR RESTAURANT OPERATOR LOSES MPV IN A...</td>\n",
       "      <td>arson</td>\n",
       "      <td>contacted ahmad due people told vehicle yard i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNEMPLOYED CHANGES PLEA OVER ARSON\\nCourt-Arso...</td>\n",
       "      <td>arson</td>\n",
       "      <td>prokhong belonging masquerading renggan foot w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JUVENILES PLEAD NOT GUILTY TO ARSON ATTEMPTS O...</td>\n",
       "      <td>arson</td>\n",
       "      <td>around hall school silaturrahim alleged mansor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>Lorry driver arrested over hit-and-run\\nSource...</td>\n",
       "      <td>traffic</td>\n",
       "      <td>attending due old run school body killed head ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>Nurse charged with causing death by reckless d...</td>\n",
       "      <td>traffic</td>\n",
       "      <td>passenger however excellent account bearing en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>`Speeding forklift summons not cancelled yet'\\...</td>\n",
       "      <td>traffic</td>\n",
       "      <td>wong contacted cancelled police bearing end st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>Driver: I was not speeding\\nSource:New Straits...</td>\n",
       "      <td>traffic</td>\n",
       "      <td>end driving emerged stuck overturning several ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>JAPAN: HIT-AND-RUN ACCIDENT VICTIM IN OKINAWA ...</td>\n",
       "      <td>traffic</td>\n",
       "      <td>base masahiko driving thereupon school week re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2251 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_article    label  \\\n",
       "0     POLICE INVESTIGATE ARSON CLAIM IN FIRE THAT TO...    arson   \n",
       "1     JAPAN: KINKAKUJI TEMPLE HOLDS CEREMONY RECALLI...    arson   \n",
       "2     NASI KANDAR RESTAURANT OPERATOR LOSES MPV IN A...    arson   \n",
       "3     UNEMPLOYED CHANGES PLEA OVER ARSON\\nCourt-Arso...    arson   \n",
       "4     JUVENILES PLEAD NOT GUILTY TO ARSON ATTEMPTS O...    arson   \n",
       "...                                                 ...      ...   \n",
       "2246  Lorry driver arrested over hit-and-run\\nSource...  traffic   \n",
       "2247  Nurse charged with causing death by reckless d...  traffic   \n",
       "2248  `Speeding forklift summons not cancelled yet'\\...  traffic   \n",
       "2249  Driver: I was not speeding\\nSource:New Straits...  traffic   \n",
       "2250  JAPAN: HIT-AND-RUN ACCIDENT VICTIM IN OKINAWA ...  traffic   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0     week several gunung people told work white hos...  \n",
       "1     kyodo day old july hold monk official charge t...  \n",
       "2     contacted ahmad due people told vehicle yard i...  \n",
       "3     prokhong belonging masquerading renggan foot w...  \n",
       "4     around hall school silaturrahim alleged mansor...  \n",
       "...                                                 ...  \n",
       "2246  attending due old run school body killed head ...  \n",
       "2247  passenger however excellent account bearing en...  \n",
       "2248  wong contacted cancelled police bearing end st...  \n",
       "2249  end driving emerged stuck overturning several ...  \n",
       "2250  base masahiko driving thereupon school week re...  \n",
       "\n",
       "[2251 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = CountVectorizer(max_features=10000)\n",
    "train_x = tf.fit_transform(df[\"preprocessed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2251x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 227971 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132 205 244]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import find\n",
    "\n",
    "print(find(train_x)[0][np.where(find(train_x)[1]==0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import find\n",
    "from bisect import bisect_left\n",
    "\n",
    "class NaiveBayes(BaseEstimator, TransformerMixin): \n",
    "\n",
    "    def __init__(self):        \n",
    "        self._feats_prob = {}\n",
    "        self._alpha =1E-20\n",
    "\n",
    "    def _array1D(self, size):       return np.zeros(size, dtype=float) \n",
    "    \n",
    "    def get_features(self):\n",
    "        return np.array([k for k in self._feats_prob.keys()])\n",
    "    \n",
    "    def fit(self, X, y):          \n",
    "        y_ = np.array(copy.deepcopy(y))\n",
    "        self._labels = y.unique() # unique labels as an array\n",
    "        self._labelSize = len(self._labels)\n",
    "        feats_all = find(X)[1] # representation of all features\n",
    "        examples_all = find(X)[0]  # representation of all documents \n",
    "        feats = set(feats_all)\n",
    "        examples = set(examples_all)\n",
    "        exampleSize = len(examples)   \n",
    "#         labeln = len(self._labels) #size of labels  \n",
    "        labelExamples = [set()] * self._labelSize\n",
    " \n",
    "        for i in range(self._labelSize):\n",
    "            labelExamples[i] = [j for j in examples_all if y_[j]==self._labels[i]]\n",
    "        probs = []    \n",
    "        for feat in feats:\n",
    "            prob = self._array1D(self._labelSize)\n",
    "            for label in range(self._labelSize):\n",
    "                occurances = len(set(examples_all[np.where(feats_all == feat)]).intersection(labelExamples[label] ))\n",
    "                 \n",
    "                prob[label] = (occurances+self._alpha)* len(labelExamples[i])/exampleSize\n",
    "            probs.append(prob)\n",
    "\n",
    "\n",
    "        self._feats_prob = dict(zip(feats, probs)) #probability of features assigned to each label\n",
    "    \n",
    "    def _pred_oneEx(self,X,x, feats_probs):\n",
    "        # X: sparse matrix of big test data/new exmaples\n",
    "        # x: current index\n",
    "        max_prob = -0.001\n",
    "        pred_label = -1\n",
    "        probs = [np.prod([feats_probs.get(feat)[label] for feat in X[x].indices])\n",
    "                for label in range(len(self._labels))]\n",
    "        return self._labels[probs.index(max(probs))]\n",
    "    \n",
    "    def predict(self,X):\n",
    "              \n",
    "        return [self._pred_oneEx(X,x, self._feats_prob) for x in set(find(X)[0])]\n",
    "    \n",
    "    def evaluate_chromo(self, X,y, chromo):\n",
    "        ones = [1]*len(self._labels)\n",
    "        chromo_feats = self.get_features()[np.where(chromo==1)]\n",
    "        chromo_feats_prob = copy.deepcopy(self._feats_prob)\n",
    "        for feat in chromo_feats:\n",
    "            chromo_feats_prob[feat] = ones\n",
    "            \n",
    "        lbls =  [self._pred_oneEx(X,x, chromo_feats_prob) for x in set(find(X)[0])]\n",
    "        return f1_score(lbls, y, average=\"macro\")\n",
    "    \n",
    "    def get_optimal_feat(self, chromo): \n",
    "        self._feats_prob = self._feats_prob[np.where(current_solution==1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivebayes = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivebayes.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elaine.khoosynnyie\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "lbls = naivebayes.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9526414943497565"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(lbls, train_y, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def optimize( x_valid,  y_valid, model, max_temp ,temp_decrement, interval):\n",
    "    \n",
    "        '''intialize a solution, the solution will be a binary vector in the form\n",
    "            10011000011110000001110101111000010111000101...........0110\n",
    "          the length of the solution will be equivelent to the length of our vocabulary\n",
    "          1 : means the corresponding word in the vocabolary is selected\n",
    "          0 : otherwise\n",
    "        '''\n",
    "        def initial_solution():\n",
    "            return np.array([random.randint(0, 1)  for _ in range(len(model.get_features()))])\n",
    "\n",
    " \n",
    "        def create_new_solution(current_solution):\n",
    "            # SA algo\n",
    "            new_solution = current_solution.copy()\n",
    "            sol_length = len(new_solution) # array of 0,1 (chromosomes)\n",
    "            number_of_neighbours = random.randint(int(sol_length / 5), int(sol_length / 3))\n",
    "            neighbours = set([random.randint(0, sol_length - 1) for _ in range(number_of_neighbours)])\n",
    "            for neighbour in neighbours: new_solution[neighbour] = 1 - new_solution[neighbour]\n",
    "            return np.array(new_solution)\n",
    "       \n",
    "        \n",
    "        def evaluate_new_solution(solution ):\n",
    "          \n",
    "            return model.evaluate_chromo(x_valid, y_valid, solution)\n",
    "            \n",
    "\n",
    "        current_solution = initial_solution()\n",
    "        it  = 0\n",
    "        T = max_temp\n",
    "        f_initial = evaluate_new_solution( current_solution)\n",
    "        while (f_initial<1.0 and T>1) :\n",
    "            \n",
    "            new_solution = create_new_solution(current_solution)\n",
    "            \n",
    "            f_gener = evaluate_new_solution( new_solution)\n",
    "            if (f_initial > f_gener):\n",
    "                p = math.exp((f_initial - f_gener) / (abs(T)+1))\n",
    "                if p < np.random.random(): current_solution = new_solution.copy()\n",
    "            elif (f_initial < f_gener):\n",
    "                current_solution = new_solution.copy()\n",
    "                f_initial = f_gener\n",
    "                print('f1={} , temperature={}'.format(round(f_gener,5), round(T, 3)))\n",
    "            if it % interval == 0:\n",
    "                T = T*temp_decrement\n",
    "            \n",
    "            it += 1\n",
    "        return  current_solution #Chromosomes of optimal solution only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=0.95503 , temperature=100\n",
      "f1=0.9559 , temperature=80.0\n",
      "f1=0.95631 , temperature=80.0\n",
      "f1=0.9566 , temperature=64.0\n",
      "f1=0.95733 , temperature=26.214\n",
      "f1=0.95883 , temperature=16.777\n",
      "f1=0.96011 , temperature=13.422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_sol=optimize( train_x,  train_y, naivebayes, 100 ,0.8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Translate chromosomes into naive bayes features\n",
    "naive_bayes.get_optimal_feat(optimal_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into train (fit) validation (optimize) test (test) sets\n",
    "# TODO: Tackling overfitting due to too many iterations "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
