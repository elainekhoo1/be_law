# be_law
Repository for Learning and Wellbeing 2021 (Mentor: Bassam Al-Salemi, PhD; Mentee: Elaine K.)

Download pretrained Universal Sentence Encoder at https://tfhub.dev/google/universal-sentence-encoder-large/5
The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. The universal-sentence-encoder-large model is trained with a Transformer encoder.

